
@inproceedings{BreitingerKMM20,
  ids = {BreitingerKMM20a},
  title = {Supporting the {{Exploration}} of {{Semantic Features}} in {{Academic Literature}} using {{Graph}}-based {{Visualizations}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Breitinger, Corinna and Kolcu, Birkan and Meuschke, Monique and Meuschke, Norman and Gipp, Bela},
  year = {2020},
  month = aug,
  copyright = {All rights reserved},
  keywords = {!bg,!bg_author,!cb,!cb_author,!nm,!nm_author,\#nosource,jabref_imp1_clean},
  oldkey = {Breitinger2020},
  topic = {rec}
}

@inproceedings{DahmSMG17,
  ids = {DahmSMG17a},
  title = {A {{Vision}} for {{Performing Social}} and {{Economic Data Analysis}} using {{Wikipedia}}'s {{Edit History}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web Companion}}},
  author = {Dahm, Erik and Schubotz, Moritz and Meuschke, Norman and Gipp, Bela},
  year = {2017},
  month = apr,
  pages = {1627--1634},
  publisher = {{ACM}},
  doi = {10.1145/3041021.3053363},
  url = {http://doi.acm.org/10.1145/3041021.3053363},
  abstract = {In this vision paper, we suggest combining two lines of research to study the collective behavior of Wikipedia contributors. The first line of research analyzes Wikipedia's edit history to quantify the quality of individual contributions and the resulting reputation of the contributor. The second line of research surveys Wikipedia contributors to gain insights, e.g., on their personal and professional background, socioeconomic status, or motives to contribute toWikipedia. While both lines of research are valuable on their own, we argue that the combination of both approaches could yield insights that exceed the sum of the individual parts. Linking survey data to contributor reputation and content-based quality metrics could provide a large-scale, public domain data set to perform user modeling, i.e. deducing interest profiles of user groups. User profiles can, among other applications, help to improve recommender systems. The resulting dataset can also enable a better understanding and improved prediction of high quality Wikipedia content and successfulWikipedia contributors. Furthermore, the dataset can enable novel research approaches to investigate team composition and collective behavior as well as help to identify domain experts and young talents. We report on the status of implementing our large-scale, content-based analysis of the Wikipedia edit history using the big data processing framework Apache Flink. Additionally, we describe our plans to conduct a survey among Wikipedia contributors to enhance the content-based quality metrics.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\K439EXGT\\DahmSMG17--tr--a_vision_for_performing_social_and_economic_data_analysis_using_wikipedias_edit_history.pdf;D\:\\Zotero\\nmeuschke\\Data\\storage\\QI9878ZS\\DahmSMG17--tr--a_vision_for_performing_social_and_economic_data_analysis_using_wikipedias_edit_history.pdf},
  isbn = {978-1-4503-4914-7},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint,wikipedia},
  oldkey = {Dahm2017},
  preprint = {https://ag-gipp.github.io/bib/preprints/dahm2017.pdf},
  topic = {wiki}
}

@article{FoltynekMG19,
  ids = {FoltynekMG19a},
  title = {Academic {{Plagiarism Detection}}: {{A Systematic Literature Review}}},
  author = {Foltýnek, Tomáš and Meuschke, Norman and Gipp, Bela},
  year = {2019},
  month = oct,
  volume = {52},
  pages = {112:1-112:42},
  issn = {0360-0300},
  doi = {10.1145/3345317},
  abstract = {This article summarizes the research on computational methods to detect academic plagiarism by systematically reviewing 239 research papers published between 2013 and 2018. To structure the presentation of the research contributions, we propose novel technically oriented typologies for plagiarism prevention and detection efforts, the forms of academic plagiarism, and computational plagiarism detection methods. We show that academic plagiarism detection is a highly active research field. Over the period we review, the field has seen major advances regarding the automated detection of strongly obfuscated and thus hard-to-identify forms of academic plagiarism. These improvements mainly originate from better semantic text analysis methods, the investigation of non-textual content features, and the application of machine learning. We identify a research gap in the lack of methodologically thorough performance evaluations of plagiarism detection systems. Concluding from our analysis, we see the integration of heterogeneous analysis methods for textual and non-textual content features using machine learning as the most promising area for future research contributions to improve the detection of academic plagiarism further.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\6FERIXS4\\FoltynekMG19--NM--academic_plagiarism_detection_a_systematic_literature_review.pdf},
  journal = {ACM Computing Surveys},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev},
  number = {6},
  oldkey = {Foltynek2019},
  topic = {pd}
}

@incollection{FoltynekRSM20,
  ids = {FoltynekRSM20a},
  title = {Detecting {{Machine}}-{{Obfuscated Plagiarism}}},
  booktitle = {Sustainable {{Digital Communities}}},
  author = {Foltýnek, Tomáš and Ruas, Terry and Scharpf, Philipp and Meuschke, Norman and Schubotz, Moritz and Grosky, William and Gipp, Bela},
  editor = {Sundqvist, Anneli and Berget, Gerd and Nolin, Jan and Skjerdingstad, Kjell Ivar},
  year = {2020},
  month = mar,
  volume = {12051 LNCS},
  pages = {816--827},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-43687-2_68},
  url = {http://link.springer.com/10.1007/978-3-030-43687-2_68},
  abstract = {Research on academic integrity has identified online paraphrasing tools as a severe threat to the effectiveness of plagiarism detection systems. To enable the automated identification of machine-paraphrased text, we make three contributions. First, we evaluate the effectiveness of six prominent word embedding models in combination with five classifiers for distinguishing human-written from machine-paraphrased text. The best performing classification approach achieves an accuracy of 99.0\% for documents and 83.4\% for paragraphs. Second, we show that the best approach outperforms human experts and established plagiarism detection systems for these classification tasks. Third, we provide a Web application that uses the best performing classification approach to indicate whether a text underwent machine-paraphrasing. The data and code of our study are openly available.},
  copyright = {Creative Commons Attribution-ShareAlike 4.0 International License (CC-BY-SA)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\NW82EY3I\\FoltynekRSM20--tr--detecting_machine-obfuscated_plagiarism.pdf},
  isbn = {978-3-030-43686-5 978-3-030-43687-2},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean},
  language = {en},
  oldkey = {Foltynek2020},
  topic = {pd}
}

@inproceedings{FoltynekVMD20,
  title = {Cross-{{Language Source Code Plagiarism Detection}} using {{Explicit Semantic Analysis}} and {{Scored Greedy String Tilling}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Foltynek, Tomas and Vsiansky, Richard and Meuschke, Norman and Dlabolova, Dita and Gipp, Bela},
  year = {2020},
  month = aug,
  doi = {10.1145/3383583.3398594},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\ZWGKINTS\\FoltynekVMD20--tr--cross-language_source_code_plagiarism_detection_using_explicit_semantic_analysis.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,⚠️ Invalid DOI},
  topic = {pd}
}

@inproceedings{GippBMB17,
  ids = {GippBMB17a},
  title = {{{CryptSubmit}}: {{Introducing Securely Timestamped Manuscript Submission}} and {{Peer Review Feedback Using}} the {{Blockchain}}},
  booktitle = {Proceedings of the 17th {{Annual International ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Gipp, Bela and Breitinger, Corinna and Meuschke, Norman and Beel, Joeran},
  year = {2017},
  month = jun,
  pages = {1--4},
  address = {{Toronto, Canada}},
  doi = {10.1109/jcdl.2017.7991588},
  abstract = {Manuscript submission systems are a central fixture in scholarly publishing. However, with existing systems, researchers must trust that their yet unpublished findings will not prematurely be disseminated due to technical weaknesses and that anonymous peer reviewers or committee members will not plagiarize unpublished content. To address this limitation, we present CryptSubmit - a system that automatically creates a decentralized, tamperproof, and publicly verifiable timestamp for each submitted manuscript by utilizing the blockchain of the cryptocurrency Bitcoin. The publicly accessible and tamperproof infrastructure of the blockchain allows researchers to independently verify the validity of the timestamp associated with their manuscript at the time of submission to a conference or journal. Our system supports researchers in protecting their intellectual property even in the face of vulnerable submission platforms or dishonest peer reviewers. Optionally, the system also generates trusted timestamps for the feedback shared by peer reviewers to increase the traceability of ideas. CryptSubmit integrates these features into the open source conference management system OJS. In the future, the method could be integrated at nearly no overhead cost into other manuscript submission systems, such as EasyChair, ConfTool, or Ambra. The introduced method can also improve electronic pre-print services and storage systems for research data.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\ELWLCSYH\\GippBMB17--tr--cryptsubmit_introducing_securely_timestamped_manuscript_submission_and_peer_review.pdf},
  isbn = {978-1-5386-3861-3},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!jb,!jb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Gipp2017b},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp2017b.pdf},
  topic = {blockchain}
}

@inproceedings{GippM11,
  ids = {GippM11a},
  title = {Citation {{Pattern Matching Algorithms}} for {{Citation}}-based {{Plagiarism Detection}}: {{Greedy Citation Tiling}}, {{Citation Chunking}} and {{Longest Common Citation Sequence}}},
  booktitle = {Proceedings of the 11th {{ACM Symposium}} on {{Document Engineering}}},
  author = {Gipp, Bela and Meuschke, Norman},
  year = {2011},
  month = sep,
  pages = {249--258},
  publisher = {{ACM}},
  address = {{Mountain, View, CA, USA}},
  doi = {10.1145/2034691.2034741},
  abstract = {Plagiarism Detection Systems have been developed to locate instances of plagiarism e.g. within scientific papers. Studies have shown that the existing approaches deliver reasonable results in identifying copy\&paste plagiarism, but fail to detect more sophisticated forms such as paraphrased plagiarism, translation plagiarism or idea plagiarism. The authors of this paper demonstrated in recent studies that the detection rate can be significantly improved by not only relying on text analysis, but by additionally analyzing the citations of a document. Citations are valuable language independent markers that are similar to a fingerprint. In fact, our examinations of real world cases have shown that the order of citations in a document often remains similar even if the text has been strongly paraphrased or translated in order to disguise plagiarism. This paper introduces three algorithms and discusses their suitability for the purpose of citation-based plagiarism detection. Due to the numerous ways in which plagiarism can occur, these algorithms need to be versatile. They must be capable of detecting transpositions, scaling and combinations in a local and global form. The algorithms are coined Greedy Citation Tiling, Citation Chunking and Longest Common Citation Sequence. The evaluation showed that if these algorithms are combined, common forms of plagiarism can be detected reliably. ? 2011 ACM.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\3BBFCZSV\\GippM11--NM--citation_pattern_matching_algorithms_for_citation-based_plagiarism_detection_greedy.pdf},
  isbn = {978-1-4503-0863-2},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {Gipp11c},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp11c.pdf},
  topic = {pd}
}

@inproceedings{GippMB11,
  ids = {GippMB11a},
  title = {Comparative {{Evaluation}} of {{Text}}- and {{Citation}}-based {{Plagiarism Detection Approaches}} using {{GuttenPlag}}},
  booktitle = {Proceedings of 11th {{Annual International ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Gipp, Bela and Meuschke, Norman and Beel, Joeran},
  year = {2011},
  month = jun,
  pages = {255--258},
  address = {{Ottawa, Canada}},
  doi = {10.1145/1998076.1998124},
  abstract = {Various approaches for plagiarism detection exist. All are based on more or less sophisticated text analysis methods such as string matching, fingerprinting or style comparison. In this paper a new approach called Citation-based Plagiarism Detection is evaluated using a doctoral thesis [8], in which a volunteer crowd-sourcing project called GuttenPlag [1] identified substantial amounts of plagiarism through careful manual inspection. This new approach is able to identify similar and plagiarized documents based on the citations used in the text. It is shown that citation-based plagiarism detection performs significantly better than text-based procedures in identifying strong paraphrasing, translation and some idea plagiarism. Detection rates can be improved by combining citation-based with text-based plagiarism detection.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\W9WJZ5E7\\GippMB11--MS--comparative_evaluation_of_text-_and_citation-based_plagiarism_detection_approaches.pdf},
  isbn = {978-1-4503-0744-4},
  keywords = {!bg,!bg_author,!bg_preprint,!jb,!jb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {Gipp11},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp11.pdf},
  topic = {pd}
}

@article{GippMB14,
  ids = {GippMB14a},
  title = {Citation-based {{Plagiarism Detection}}: {{Practicability}} on a {{Large}}-{{Scale Scientific Corpus}}},
  author = {Gipp, Bela and Meuschke, Norman and Breitinger, Corinna},
  year = {2014},
  month = aug,
  volume = {65},
  pages = {1527--1540},
  issn = {2330-1635},
  doi = {10.1002/asi.23228},
  abstract = {The automated detection of plagiarism is an information retrieval task of increasing importance as the volume of readily accessible information on the web expands. A major shortcoming of current automated plagiarism detection approaches is their dependence on high character-based similarity. As a result, heavily disguised plagiarism forms, such as paraphrases, translated plagiarism, or structural and idea plagiarism, remain undetected. A recently proposed language-independent approach to plagiarism detection, Citation-based Plagiarism Detection (CbPD), allows the detection of semantic similarity even in the absence of text overlap by analyzing the citation placement in a document's full text to determine similarity. This article evaluates the performance of CbPD in detecting plagiarism with various degrees of disguise in a collection of 185,000 biomedical articles. We benchmark CbPD against two character-based detection approaches using a ground truth approximated in a user study. Our evaluation shows that the citation-based approach achieves superior ranking performance for heavily disguised plagiarism forms. Additionally, we demonstrate CbPD to be computationally more efficient than character-based approaches. Finally, upon combining the citation-based with the traditional character-based document similarity visualization methods in a hybrid detection prototype, we observe a reduction in the required user effort for document verification.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\82V4G45M\\GippMB14--MS--citation-based_plagiarism_detection_practicability_on_a_large-scale_scientific_corpus.pdf},
  journal = {Journal of the Association for Information Science and Technology},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  number = {8},
  oldkey = {Gipp13b},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp13b.pdf},
  topic = {pd}
}

@inproceedings{GippMBB16,
  ids = {GippMBB17},
  title = {Using the {{Blockchain}} of {{Cryptocurrencies}} for {{Timestamping Digital Cultural Heritage}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Web Archiving}} and {{Digital Libraries}} ({{WADL}}) held in conjunction with the 16th {{ACM}}/{{IEEE}}-{{CS Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Gipp, Bela and Meuschke, Norman and Beel, Joeran and Breitinger, Corinna},
  year = {2016},
  volume = {13},
  pages = {1--3},
  doi = {10.5281/zenodo.3547510},
  abstract = {The proportion of information that is exclusively available online is continuously increasing. Unlike physical print media, online news outlets, magazines, or blogs are not immune to retrospective modification. Even significant editing of text in online news sources can easily go unnoticed. This poses a challenge to the preservation of digital cultural heritage. It is nearly impossible for regular readers to verify whether the textual content they encounter online has at one point been modified from its initial state, and at what time or to what extent the text was modified to its current version. In this paper, we propose a web-based platform that allows users to submit the URL for any web content they wish to track for changes. The system automatically creates a trusted timestamp stored in the blockchain of the cryptocurrency Bitcoin for the hash of the HTML content available at the user-specified URL. By using trusted timestamping to secure a ‘snapshot’ of online information as it existed at a specific time, any subsequent changes made to the content can be identified.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\T2RIZ5ER\\GippMBB16--MS--using_the_blockchain_of_cryptocurrencies_for_timestamping_digital_cultural_heritage.pdf},
  keywords = {!bg,!bg_author,!cb,!cb_author,!jb,!jb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Gipp2017a},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp2017a.pdf},
  topic = {blockchain}
}

@inproceedings{GippMBL13,
  ids = {GippMBL13a},
  title = {Demonstration of the {{First Citation}}-based {{Plagiarism Detection Prototype}}},
  booktitle = {Proceedings of the 36th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Gipp, Bela and Meuschke, Norman and Breitinger, Corinna and Lipinski, Mario and Nürnberger, Andreas},
  year = {2013},
  month = jul,
  pages = {1119--1120},
  publisher = {{ACM}},
  address = {{Dublin, UK}},
  doi = {10.1145/2484028.2484214},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\TXME7VNL\\GippMBL13--NM--demonstration_of_the_first_citation-based_plagiarism_detection_prototype.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint},
  oldkey = {Gipp13},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp13.pdf},
  topic = {pd}
}

@inproceedings{GippMBP14,
  ids = {GippMBP14a},
  title = {Web-based {{Demonstration}} of {{Semantic Similarity Detection Using Citation Pattern Visualization}} for a {{Cross Language Plagiarism Case}}},
  booktitle = {Proceedings {{International Conference}} on {{Enterprise Information Systems}} ({{ICEIS}})},
  author = {Gipp, Bela and Meuschke, Norman and Breitinger, Corinna and Pitman, Jim and Nürnberger, Andreas},
  year = {2014},
  month = apr,
  volume = {2},
  pages = {677--683},
  address = {{Lisbon, Portugal}},
  doi = {10.5220/0004985406770683},
  abstract = {In a previous paper, we showed that analyzing citation patterns in the well-known plagiarized thesis by K. T. zu Guttenberg clearly outperformed current detection methods in identifying cross-language plagiarism. However, the experiment was a proof of concept and we did not provide a prototype. This paper presents a fully functional, web-based visualization of citation patterns for this verified cross-language plagiarism case, allowing the user to interactively experience the benefits of citation pattern analysis for plagiarism detection. Using examples from the Guttenberg plagiarism case, we demonstrate that the citation pattern visualization reduces the required examiner effort to verify the extent of plagiarism. Copyright ? 2014 SCITEPRESS - Science and Technology Publications.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\PSXJ4YNC\\GippMBP14--NM--web-based_demonstration_of_semantic_similarity_detection_using_citation_pattern_visualization.pdf},
  isbn = {978-989-758-028-4},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {gipp14a},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp14a.pdf},
  topic = {pd}
}

@inproceedings{GippMG15iConfTT,
  ids = {GippMG15iConfTT},
  title = {Decentralized {{Trusted Timestamping}} using the {{Crypto Currency Bitcoin}}},
  booktitle = {Proceedings of the {{iConference}} 2015},
  author = {Gipp, Bela and Meuschke, Norman and Gernandt, Andre},
  year = {2015},
  month = mar,
  address = {{Newport Beach, California}},
  doi = {10.5281/zenodo.3547488},
  url = {http://ischools.org/the-iconference/},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\WY24VR3L\\GippMG15iConfTT--tr--decentralized_trusted_timestamping_using_the_crypto_currency_bitcoin.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!nm_author,!nm_preprint,\#nosource,jabref_imp2,old_tex_field_preprint},
  oldkey = {Gipp15a},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp15a.pdf},
  topic = {blockchain}
}

@inproceedings{GippML15,
  ids = {GippML15a},
  title = {{{CITREC}}: {{An Evaluation Framework}} for {{Citation}}-{{Based Similarity Measures}} based on {{TREC Genomics}} and {{PubMed Central}}},
  booktitle = {Proceedings of the {{iConference}}},
  author = {Gipp, Bela and Meuschke, Norman and Lipinski, Mario},
  year = {2015},
  month = mar,
  address = {{Newport Beach, California}},
  doi = {10.5281/zenodo.3547372},
  url = {http://hdl.handle.net/2142/73680},
  abstract = {Citation-based similarity measures such as Bibliographic Coupling and Co-Citation are an integral component of many information retrieval systems. However, comparisons of the strengths and weaknesses of measures are challenging due to the lack of suitable test collections. This paper presents CITREC, an open evaluation framework for citation-based and text-based similarity measures. CITREC prepares the data from the PubMed Central Open Access Subset and the TREC Genomics collection for a citation-based analysis and provides tools necessary for performing evaluations of similarity measures. To account for different evaluation purposes, CITREC implements 35 citation-based and text-based similarity measures, and features two gold standards. The first gold standard uses the Medical Subject Headings (MeSH) thesaurus and the second uses the expert relevance feedback that is part of the TREC Genomics collection to gauge similarity. CITREC additionally offers a system that allows creating user defined gold standards to adapt the evaluation framework to individual information needs and evaluation purposes.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\28SNMCNV\\GippML15--tr--citrec_an_evaluation_framework_for_citation-based_similarity_measures_based_on_trec.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Gipp15b},
  preprint = {https://ag-gipp.github.io/bib/preprints/gipp15b.pdf},
  topic = {cit}
}

@inproceedings{HamborgMAG17,
  ids = {HamborgMAG17a},
  title = {Identification and {{Analysis}} of {{Media Bias}} in {{News Articles}}},
  booktitle = {Proceedings of the 15th {{International Symposium}} of {{Information Science}}},
  author = {Hamborg, Felix and Meuschke, Norman and Aizawa, Akiko and Gipp, Bela},
  editor = {Gaede, Maria and Trkulja, Violeta and Petra, Vivien},
  year = {2017},
  month = mar,
  pages = {224--236},
  address = {{Berlin}},
  doi = {10.18452/1446},
  url = {http://edoc.hu-berlin.de/docviews/abstract.php?id=43364},
  abstract = {Depending on the news source, a reader can be exposed to a different narrative and conflicting perceptions for the same event. Today, news aggregators help users cope with the large volume of news published daily. However, aggregators focus on presenting shared information, but do not expose the different perspectives from articles on same topics. Thus, users of such aggregators suffer from media bias, which is often implemented intentionally to influence public opinion. In this paper, we present NewsBird, an aggregator that presents shared and different information on topics. Currently, NewsBird reveals different perspectives on international news. Our system has led to insights about media bias and news analysis, which we use to propose approaches to be investigated in future research. Our vision is to provide a system that reveals media bias, and thus ultimately allows users to make their own judgement on the potential bias inherent in news.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\TTDWTVDL\\HamborgMAG17--tr--identification_and_analysis_of_media_bias_in_news_articles.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!fh,!fh_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Hamborg2017a},
  preprint = {https://ag-gipp.github.io/bib/preprints/hamborg2017a.pdf},
  topic = {newsanalysis}
}

@inproceedings{HamborgMBG17,
  title = {news-please: {{A Generic News Crawler}} and {{Extractor}}},
  booktitle = {Proceedings of the 15th {{International Symposium}} of {{Information Science}}},
  author = {Hamborg, Felix and Meuschke, Norman and Breitinger, Corinna and Gipp, Bela},
  editor = {Gaede, Maria and Trkulja, Violeta and Petra, Vivien},
  year = {2017},
  month = mar,
  pages = {218--223},
  address = {{Berlin}},
  url = {http://edoc.hu-berlin.de/docviews/abstract.php?id=43365},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\UD3TMYMD\\HamborgMBG17--tr--news-please_a_generic_news_crawler_and_extractor.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!fh,!fh_author,!nm_author,!nm_preprint,jabref_imp2,old_tex_field_preprint},
  oldkey = {Hamborg2017},
  preprint = {https://ag-gipp.github.io/bib/preprints/hamborg2017.pdf},
  topic = {newsanalysis}
}

@inproceedings{HamborgMG17,
  ids = {HamborgMG17a},
  title = {Matrix-{{Based News Aggregation}}: {{Exploring Different News Perspectives}}},
  booktitle = {Proceedings of the 17th {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
  year = {2017},
  month = jun,
  pages = {1--10},
  doi = {10.1109/jcdl.2017.7991561},
  abstract = {News aggregators capably handle the large amount of news that is published nowadays. However, these systems focus on the presentation of important, common information in news, but do not reveal different perspectives on the same topic. Thus, current news aggregators suffer from media bias, i.e. differences in the content or presentation of news. Finding such differences is crucial to reduce the effects of media bias. This paper presents matrix-based news analysis (MNA), a novel design for news exploration. MNA helps users gain a broad and diverse news understanding by presenting various news perspectives on the same topic. Furthermore, we present NewsBird, a news aggregator that implements MNA to find different perspectives on international news topics. The results of a case study demonstrate that NewsBird broadens the user's news understanding while it also provides similar news aggregation functionalities as established systems.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\Z4HMN8VH\\HamborgMG17--tr--matrix-based_news_aggregation_exploring_different_news_perspectives.pdf},
  isbn = {978-1-5386-3861-3},
  keywords = {!bg,!bg_author,!bg_preprint,!fh,!fh_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Hamborg2017b},
  preprint = {https://ag-gipp.github.io/bib/preprints/hamborg2017b.pdf},
  topic = {newsanalysis}
}

@article{HamborgMG18,
  ids = {HamborgMG18a},
  title = {Bias-aware {{News Analysis Using Matrix}}-based {{News Aggregation}}},
  author = {Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
  year = {2018},
  month = may,
  publisher = {{Springer Berlin Heidelberg}},
  issn = {1432-5012, 1432-1300},
  doi = {10.1007/s00799-018-0239-9},
  url = {http://link.springer.com/10.1007/s00799-018-0239-9},
  abstract = {Media bias describes differences in the content or presentation of news. It is an ubiquitous phenomenon in news coverage that can have severely negative effects on individuals and society. Identifying media bias is a challenging problem, for which current information systems offer little support. News aggregators are the most important class of systems to support users in coping with the large amount of news that is published nowadays. These systems focus on identifying and presenting important, common information in news articles, but do not reveal different perspectives on the same topic. Due to this analysis approach, current news aggregators cannot effectively reveal media bias. To address this problem, we present matrix-based news aggregation, a novel approach for news exploration that helps users gain a broad and diverse news understanding by presenting various perspectives on the same news topic. Additionally, we present NewsBird, an open-source news aggregator that implements matrix-based news aggregation for international news topics. The results of a user study showed that NewsBird more effectively broadens the user’s news understanding than the list-based visualization approach employed by established news aggregators, while achieving comparable effectiveness and efficiency for the two main use cases of news consumption: getting an overview of and finding details on current news topics.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\3Y238X2A\\HamborgMG18--MS--bias-aware_news_analysis_using_matrix-based_news_aggregation.pdf},
  journal = {International Journal on Digital Libraries (IJDL)},
  keywords = {!bg,!bg_author,!bg_preprint,!fh,!fh_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Hamborg2018b},
  preprint = {https://ag-gipp.github.io/bib/preprints/hamborg2018b.pdf},
  topic = {newsanalysis}
}

@inproceedings{IhleSMG20,
  ids = {IhleSMG20a},
  title = {A {{First Step Towards Content Protecting Plagiarism Detection}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Ihle, Cornelius and Schubotz, Moritz and Meuschke, Norman and Gipp, Bela},
  year = {2020},
  month = aug,
  copyright = {All rights reserved},
  keywords = {!bg,!bg_author,!ms,!ms_author,!nm,!nm_author,\#nosource,jabref_imp1_clean},
  oldkey = {Ihle2020},
  topic = {pd}
}

@article{MeschenmoserMHG16,
  ids = {MeschenmoserMHG16},
  title = {Scraping {{Scientific Web Repositories}}: {{Challenges}} and {{Solutions}} for {{Automated Content Extraction}}},
  shorttitle = {Scraping {{Scientific Web Repositories}}},
  author = {Meschenmoser, Philipp and Meuschke, Norman and Hotz, Manuel and Gipp, Bela},
  year = {2016},
  month = oct,
  volume = {22},
  issn = {1082-9873},
  doi = {10.1045/september2016-meschenmoser},
  abstract = {Aside from improving the visibility and accessibility of scientific publications, many scientific Web repositories also assess researchers' quantitative and qualitative publication performance, e.g., by displaying metrics such as the h-index. These metrics have become important for research institutions and other stakeholders to support impactful decision making processes such as hiring or funding decisions. However, scientific Web repositories typically offer only simple performance metrics and limited analysis options. Moreover, the data and algorithms to compute performance metrics are usually not published. Hence, it is not transparent or verifiable which publications the systems include in the computation and how the systems rank the results. Many researchers are interested in accessing the underlying scientometric raw data to increase the transparency of these systems. In this paper, we discuss the challenges and present strategies to programmatically access such data in scientific Web repositories. We demonstrate the strategies as part of an open source tool (MIT license) that allows research performance comparisons based on Google Scholar data. We would like to emphasize that the scraper included in the tool should only be used if consent was given by the operator of a repository. In our experience, consent is often given if the research goals are clearly explained and the project is of a non-commercial nature.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\8KYXE9ZS\\MeschenmoserMHG16--tr--scraping_scientific_web_repositories_challenges_and_solutions_for_automated_content.pdf},
  journal = {D-Lib Magazine},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  language = {en},
  number = {9/10},
  oldkey = {Meschenmoser2016a},
  preprint = {https://ag-gipp.github.io/bib/preprints/meschenmoser2016a.pdf},
  topic = {misc}
}

@inproceedings{MeschenmoserMHG16WOSP,
  title = {Scraping {{Scientific Web Repositories}}: {{Challenges}} and {{Solutions}} for {{Automated Content Extraction}}},
  booktitle = {Proceedings of the 5th {{International Workshop}} on {{Mining Scientific Publications}} ({{WOSP}}) held in conjunction with the 16th {{ACM}}/{{IEEE}}-{{CS Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Meschenmoser, Philipp and Meuschke, Norman and Hotz, Manuel and Gipp, Bela},
  year = {2016},
  address = {{Newark, New Jersey, USA}},
  doi = {10.1045/september2016-meschenmoser},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\CUVWJAFJ\\MeschenmoserMHG16WOSP--MS--scraping_scientific_web_repositories_challenges_and_solutions_for_automated_content.pdf},
  keywords = {!bg,!bg_author,!nm_author,\#nosource,jabref_imp2,old_tex_field_preprint},
  oldkey = {Meschenmoser2016},
  preprint = {https://ag-gipp.github.io/bib/preprints/meschenmoser2016.pdf},
  topic = {misc}
}

@phdthesis{Meuschke11,
  ids = {Meuschke11a},
  title = {Citation-{{Based Plagiarism Detection}} for {{Scientific Documents}}},
  author = {Meuschke, Norman},
  year = {2011},
  month = jun,
  address = {{Magdeburg}},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\M5D9ULHP\\Meuschke11--MS--citation-based_plagiarism_detection_for_scientific_documents.pdf},
  keywords = {!nm,!nm_author,jabref_imp1_clean,nm_diss_cbpd},
  oldkey = {Meuschke11b},
  owner = {Norman},
  school = {Dep. of Computer Science, Otto-von-Guericke-University Magdeburg, Germany},
  type = {Diploma {{Thesis}}}
}

@article{MeuschkeG13,
  ids = {MeuschkeG13a},
  title = {State of the {{Art}} in {{Detecting Academic Plagiarism}}},
  author = {Meuschke, Norman and Gipp, Bela},
  year = {2013},
  month = jun,
  volume = {9},
  pages = {50--71},
  issn = {1833-2595},
  doi = {10/gf9rvh},
  url = {https://www.ojs.unisa.edu.au/index.php/IJEI/article/view/847/610},
  abstract = {The problem of academic plagiarism has been present for centuries. Yet, the widespread dissemination of information technology, including the internet, made plagiarising much easier. Consequently, methods and systems aiding in the detection of plagiarism have attracted much research within the last two decades. Researchers proposed a variety of solutions, which we will review comprehensively in this article. Available detection systems use sophisticated and highly efficient character-based text comparisons, which can reliably identify verbatim and moderately disguised copies. Automatically detecting more strongly disguised plagiarism, such as paraphrases, translations or idea plagiarism, is the focus of current research. Proposed approaches for this task include intrinsic, cross-lingual and citation-based plagiarism detection. Each method offers unique strengths and weaknesses; however, none is currently mature enough for practical use. In the future, plagiarism detection systems may benefit from combining traditional character-based detection methods with these emerging detection approaches. Introduction},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\WCED6IDE\\MeuschkeG13--NM--state_of_the_art_in_detecting_academic_plagiarism.pdf},
  journal = {International Journal for Educational Integrity},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  number = {1},
  oldkey = {Meuschke13},
  preprint = {https://ag-gipp.github.io/bib/preprints/meuschke13.pdf},
  topic = {pd}
}

@inproceedings{MeuschkeG14,
  ids = {MeuschkeG14a},
  title = {Reducing {{Computational Effort}} for {{Plagiarism Detection}} by {{Using Citation Characteristics}} to {{Limit Retrieval Space}}},
  booktitle = {Proceedings of the 14th {{Annual International ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Meuschke, Norman and Gipp, Bela},
  year = {2014},
  month = sep,
  pages = {197--200},
  address = {{London, UK}},
  doi = {10.1109/JCDL.2014.6970168},
  abstract = {This paper proposes a hybrid approach to plagiarism detection in academic documents that integrates detection methods using citations, semantic argument structure, and semantic word similarity with character-based methods to achieve a higher detection performance for disguised plagiarism forms. Currently available software for plagiarism detection exclusively performs text string comparisons. These systems find copies, but fail to identify disguised plagiarism, such as paraphrases, translations, or idea plagiarism. Detection approaches that consider semantic similarity on word and sentence level exist and have consistently achieved higher detection accuracy for disguised plagiarism forms compared to character-based approaches. However, the high computational effort of these semantic approaches makes them infeasible for use in real-world plagiarism detection scenarios. The proposed hybrid approach uses citation-based methods as a preliminary heuristic to reduce the retrieval space with a relatively low loss in detection accuracy. This preliminary step can then be followed by a computationally more expensive semantic and character-based analysis. We show that such a hybrid approach allows semantic plagiarism detection to become feasible even on large collections for the first time.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\WIKVDPVG\\MeuschkeG14--NM--reducing_computational_effort_for_plagiarism_detection_by_using_citation_characteristics.pdf},
  isbn = {978-1-4799-5569-5},
  jabref-groups = {phd-m, CbPD thesis},
  keywords = {!bg,!bg_author,!bg_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {Meuschke14},
  owner = {Norman},
  preprint = {https://ag-gipp.github.io/bib/preprints/meuschke14.pdf},
  topic = {pd}
}

@inproceedings{MeuschkeGB12,
  ids = {MeuschkeGB12a},
  title = {{{CitePlag}}: {{A Citation}}-based {{Plagiarism Detection System Prototype}}},
  booktitle = {Proceedings of the 5th {{International Plagiarism Conference}}},
  author = {Meuschke, Norman and Gipp, Bela and Breitinger, Corinna},
  year = {2012},
  month = jul,
  address = {{Newcastle upon Tyne, UK}},
  doi = {10/ggm3qb},
  abstract = {This paper presents an open-source prototype of a citation-based plagiarism detection system called CitePlag. The underlying idea of the system is to evaluate the citations of academic documents as language independent markers to detect plagiarism. CitePlag uses three different detection algorithms that analyze the citation sequence of academic documents for similar patterns that may indicate unduly used foreign text or ideas. The algorithms consider multiple citation related factors such as proximity and order of citations within the text, or their probability of co-occurrence in order to compute document similarity scores. We present technical details of CitePlag's detection algorithms and the acquisition of test data from the PubMed Central Open Access Subset. Future advancements of the prototype focus on increasing the reference database by enabling the system to process more document and citation formats. Furthermore, we aim to improve CitePlag's detection algorithms and scoring functions for reducing the number of false positives. Eventually, we plan to integrate text with citation-based detection algorithms within CitePlag.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\ASZ2AM72\\MeuschkeGB12--NM--citeplag_a_citation-based_plagiarism_detection_system_prototype.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint},
  oldkey = {Meuschke12},
  preprint = {https://ag-gipp.github.io/bib/preprints/meuschke12.pdf},
  topic = {pd}
}

@inproceedings{MeuschkeGSB18,
  ids = {MeuschkeGSB18a},
  title = {An {{Adaptive Image}}-{{Based Plagiarism Detection Approach}}},
  booktitle = {Proceedings of the 18th {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Meuschke, Norman and Gondek, Christopher and Seebacher, Daniel and Breitinger, Corinna and Keim, Daniel and Gipp, Bela},
  year = {2018},
  month = jun,
  pages = {131--140},
  address = {{Fort Worth, USA}},
  doi = {10.1145/3197026.3197042},
  abstract = {Identifying plagiarized content is a crucial task for educational and research institutions, funding agencies, and academic publishers. Plagiarism detection systems available for productive use reliably identify copied text, or near-copies of text, but oſten fail to detect disguised forms of academic plagiarism, such as paraphrases, trans- lations, and idea plagiarism. To improve the detection capabilities for disguised forms of academic plagiarism, we analyze the images in academic documents as text-independent features. We propose an adaptive, scalable, and extensible image-based plagiarism de- tection approach suitable for analyzing a wide range of image similarities that we observed in academic documents. The proposed detection approach integrates established image analysis methods, such as perceptual hashing, with newly developed similarity assess- ments for images, such as ratio hashing and position-aware OCR text matching. We evaluate our approach using 15 image pairs that are representative of the spectrum of image similarity we observed in alleged and confirmed cases of academic plagiarism. We embed the test cases in a collection of 4,500 related images from academic texts. Our detection approach achieved a recall of 0.73 and a pre- cision of 1. These results indicate that our image-based approach can complement other content-based feature analysis approaches to retrieve potential source documents for suspiciously similar con- tent from large collections. We provide our code as open source to facilitate future research on image-based plagiarism detection.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\C4PPS6DK\\MeuschkeGSB18--NM--an_adaptive_image-based_plagiarism_detection_approach.pdf},
  isbn = {978-1-4503-5178-2},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {Meuschke2018},
  preprint = {https://ag-gipp.github.io/bib/preprints/meuschke2018.pdf},
  topic = {pd}
}

@inproceedings{MeuschkeSHS17,
  ids = {MeuschkeSHS17a,MeuschkeSHS17b},
  title = {Analyzing {{Mathematical Content}} to {{Detect Academic Plagiarism}}},
  shorttitle = {Proc. {{CIKM}}},
  booktitle = {Proceedings {{ACM Conference}} on {{Information}} and {{Knowledge Management}} ({{CIKM}})},
  author = {Meuschke, Norman and Schubotz, Moritz and Hamborg, Felix and Skopal, Tomas and Gipp, Bela},
  year = {2017},
  month = nov,
  pages = {2211--2214},
  publisher = {{ACM}},
  address = {{Singapore}},
  doi = {10.1145/3132847.3133144},
  url = {http://doi.acm.org/10.1145/3132847.3133144},
  abstract = {This paper presents, to our knowledge, the first study on analyzing mathematical expressions to detect academic plagiarism. We make the following contributions. First, we investigate confirmed cases of plagiarism to categorize the similarities of mathematical content commonly found in plagiarized publications. From this investigation, we derive possible feature selection and feature comparison strategies for developing math-based detection approaches and a ground truth for our experiments. Second, we create a test collection by embedding confirmed cases of plagiarism into the NTCIR-11 MathIR Task dataset, which contains approx. 60 million mathematical expressions in 105,120 documents from arXiv.org. Third, we develop a first math-based detection approach by implementing and evaluating different feature comparison approaches using an open source parallel data processing pipeline built using the Apache Flink framework. The best performing approach identifies all but two of our real-world test cases at the top rank and achieves a mean reciprocal rank of 0.86. The results show that mathematical expressions are promising text-independent features to identify academic plagiarism in large collections. To facilitate future research on math-based plagiarism detection, we make our source code and data available. ? 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
  biburl = {https://dblp.org/rec/bib/conf/cikm/MeuschkeSHSG17},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  core = {A;Core Rank A;http://portal.core.edu.au/conf-ranks/25/},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\QXCZHBK4\\MeuschkeSHS17--NM--analyzing_mathematical_content_to_detect_academic_plagiarism.pdf},
  isbn = {978-1-4503-4918-5},
  keywords = {!bg,!bg_author,!bg_preprint,!fh,!fh_author,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {Meuschke2017b},
  owner = {norman},
  preprint = {https://ag-gipp.github.io/bib/preprints/meuschke2017b.pdf},
  topic = {pd}
}

@inproceedings{MeuschkeSSG17,
  ids = {MeuschkeSSG17a},
  title = {Analyzing {{Semantic Concept Patterns}} to {{Detect Academic Plagiarism}}},
  booktitle = {Proceedings of the {{International Workshop}} on {{Mining Scientific Publications}} ({{WOSP}}) co-located with the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Meuschke, Norman and Siebeck, Nicolas and Schubotz, Moritz and Gipp, Bela},
  year = {2017},
  month = jun,
  pages = {46--53},
  publisher = {{IEEE Computer Society}},
  address = {{Toronto, Canada}},
  doi = {10.1145/3127526.3127535},
  abstract = {Detecting academic plagiarism is a pressing problem, e.g., for educational and research institutions, funding agencies, and academic publishers. Existing plagiarism detection systems reliably identify copied text, or near copies of text, but often fail to detect disguised forms of academic plagiarism, such as paraphrases, translations, and idea plagiarism. We present Semantic Concept Pattern Analysis - an approach that performs an integrated analysis of semantic text relatedness and structural text similarity. Using 25 officially retracted academic plagiarism cases, we demonstrate that our approach can detect plagiarism that established text matching approaches would not identify. We view our approach as a promising addition to improve the detection capabilities for strong paraphrases. We plan to further improve Semantic Concept Pattern Analysis and include the approach as part of an integrated detection process that analyzes heterogeneous similarity features to better identify the many possible forms of plagiarism in academic documents.},
  biburl = {https://dblp.org/rec/bib/conf/jcdl/MeuschkeSSG17},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\DX79UQU7\\MeuschkeSSG17--NM--analyzing_semantic_concept_patterns_to_detect_academic_plagiarism.pdf},
  isbn = {978-1-4503-5388-5},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {Meuschke2017},
  preprint = {https://ag-gipp.github.io/bib/preprints/meuschke2017a.pdf}
}

@inproceedings{MeuschkeSSG18,
  ids = {MeuschkeSSG18a},
  title = {{{HyPlag}}: {{A Hybrid Approach}} to {{Academic Plagiarism Detection}}},
  booktitle = {Proceedings of the 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Meuschke, Norman and Stange, Vincent and Schubotz, Moritz and Gipp, Bela},
  year = {2018},
  month = jun,
  pages = {1321--1324},
  address = {{Ann Arbor, MI, USA}},
  doi = {10.1145/3209978.3210177},
  abstract = {Current plagiarism detection systems reliably find instances of copied and moderately altered text, but often fail to detect strong paraphrases, translations, and the reuse of non-textual content and ideas. To improve upon the detection capabilities for such concealed content reuse in academic publications, we make four contributions: i) We present the first plagiarism detection approach that combines the analysis of mathematical expressions, images, citations and text. ii) We describe the implementation of this hybrid detection approach in the research prototype HyPlag. iii) We present novel visualization and interaction concepts to aid users in reviewing content similarities identified by the hybrid detection approach. iv) We demonstrate the usefulness of the hybrid detection and result visualization approaches by using HyPlag to analyze a confirmed case of content reuse present in a retracted research publication.},
  biburl = {https://dblp.uni-trier.de/rec/bibtex/conf/sigir/MeuschkeSSG18},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  core = {A*},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\AA58I6RS\\MeuschkeSSG18--NM--hyplag_a_hybrid_approach_to_academic_plagiarism_detection.pdf},
  isbn = {978-1-4503-5657-2},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,DFG1259-1,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint},
  oldkey = {Meuschke2018a},
  preprint = {https://www.gipp.com/wp-content/papercite-data/pdf/meuschke2018a.pdf},
  topic = {pd}
}

@inproceedings{MeuschkeSSK19,
  ids = {MeuschkeSSK19a},
  title = {Improving {{Academic Plagiarism Detection}} for {{STEM Documents}} by {{Analyzing Mathematical Content}} and {{Citations}}},
  booktitle = {Proceedings of the {{Annual International ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Meuschke, Norman and Stange, Vincent and Schubotz, Moritz and Kramer, Michael and Gipp, Bela},
  year = {2019},
  month = jun,
  pages = {120--129},
  address = {{Urbana-Champaign, Illinois, USA}},
  doi = {10.1109/JCDL.2019.00026},
  abstract = {Identifying academic plagiarism is a pressing task for educational and research institutions, publishers, and funding agencies. Current plagiarism detection systems reliably find instances of copied and moderately reworded text. However, reliably detecting concealed plagiarism, such as strong paraphrases, translations, and the reuse of nontextual content and ideas is an open research problem. In this paper, we extend our prior research on analyzing mathematical content and academic citations. Both are promising approaches for improving the detection ofconcealed academic plagiarism primarily in Science, Technology, Engineering and Mathematics (STEM). We make the following contributions: i) We present a two-stage detec- tion process that combines similarity assessments of mathematical content, academic citations, and text. ii) We introduce new similar- ity measures that consider the order of mathematical features and outperform the measures in our prior research. iii) We compare the effectiveness of the math-based, citation-based, and text-based detection approaches using confirmed cases of academic plagia- rism. iv) We demonstrate that the combined analysis of math-based and citation-based content features allows identifying potentially suspicious cases in a collection of 102K STEM documents. Overall, we show that analyzing the similarity of mathematical content and academic citations is a striking supplement for conventional text- based detection approaches for academic literature in the STEM disciplines. The data and code of our study are openly available at https://purl.org/hybridPD},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  core = {0;Core Rank A*;http://portal.core.edu.au/conf-ranks/2085/},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\K96YSRDN\\MeuschkeSSK19--NM--improving_academic_plagiarism_detection_for_stem_documents_by_analyzing_mathematical.pdf},
  isbn = {978-1-72811-547-4},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,DFG1259-1,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint,pd_litrev19},
  oldkey = {Meuschke2019},
  preprint = {https://www.gipp.com/wp-content/papercite-data/pdf/meuschke2019.pdf},
  topic = {pd}
}

@inproceedings{ScharpfSYH20,
  ids = {ScharpfSYH20a},
  title = {Classification and {{Clustering}} of {{arXiv Documents}}, {{Sections}}, and {{Abstracts Comparing Encodings}} of {{Natural}} and {{Mathematical Language}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Scharpf, Philipp and Schubotz, Moritz and Youssef, Abdou and Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
  year = {2020},
  month = jun,
  copyright = {All rights reserved},
  keywords = {!bg,!bg_author,!fh,!fh_author,!ms,!ms_author,!nm,!nm_author,\#nosource,jabref_imp1_clean},
  oldkey = {Scharpf2020},
  topic = {mathir}
}

@inproceedings{SchubotzGLC16,
  ids = {SchubotzGLC16a,disSigir16},
  title = {Semantification of {{Identifiers}} in {{Mathematics}} for {{Better Math Information Retrieval}}},
  shorttitle = {Semantification of {{Identifiers}} in {{Mathematics}} for {{MIR}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Schubotz, Moritz and Grigorev, Alexey and Leich, Marcus and Cohl, Howard S. and Meuschke, Norman and Gipp, Bela and Youssef, Abdou S. and Markl, Volker},
  year = {2016},
  month = jul,
  pages = {135--144},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2911451.2911503},
  abstract = {Mathematical formulae are essential in science, but face challenges of ambiguity, due to the use of a small number of identifiers to represent an immense number of concepts. Corresponding to word sense disambiguation in Natural Language Processing, we disambiguate mathematical identifiers. By regarding formulae and natural text as one monolithic information source, we are able to extract the semantics of identifiers in a process we term Mathematical Language Processing (MLP). As scientific communities tend to establish standard (identifier) notations, we use the document domain to infer the actual meaning of an identifier. Therefore, we adapt the software development concept of namespaces to mathematical notation. Thus, we learn namespace definitions by clustering the MLP results and mapping those clusters to subject classification schemata. In addition, this gives fundamental insights into the usage of mathematical notations in science, technology, engineering and mathematics. Our gold standard based evaluation shows that MLP extracts relevant identifier-definitions. Moreover, we discover that identifier namespaces improve the performance of automated identifier-definition extraction, and elevate it to a level that cannot be achieved within the document context alone.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  core = {A*},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\72UA8MUZ\\SchubotzGLC16--NM--semantification_of_identifiers_in_mathematics_for_better_math_information_retrieval.pdf;D\:\\Zotero\\nmeuschke\\Data\\storage\\Z2M7MHKI\\SchubotzGLC16--NM--semantification_of_identifiers_in_mathematics_for_better_math_information_retrieval.pdf},
  isbn = {978-1-4503-4069-4},
  jabref-groups = {phd-m},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,definitions,identifiers,jabref_imp1_clean,mathematical information retrieval,mathematical knowledge management,mathematical language processing,mathematics,mathoid,mathosphere,MIR,MLP,namespace discovery,old_tex_field_preprint,wikipedia},
  numpages = {10},
  oldkey = {Schubotz16},
  owner = {Moritz},
  preprint = {https://ag-gipp.github.io/bib/preprints/schubotz16.pdf},
  series = {{{SIGIR}} '16},
  topic = {mathir}
}

@inproceedings{SchubotzGMT20,
  title = {Mathematical {{Formulae}} in {{Wikimedia Projects}} 2020},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Schubotz, Moritz and {Greiner-Petter}, André and Meuschke, Norman and Teschke, Olaf and Gipp, Bela},
  year = {2020},
  month = may,
  doi = {10.1145/3383583.3398557},
  url = {http://arxiv.org/abs/2003.09417},
  urldate = {2020-06-11},
  abstract = {This poster summarizes our contributions to Wikimedia's processing pipeline for mathematical formulae. We describe how we have supported the transition from rendering formulae as course-grained PNG images in 2001 to providing modern semantically enriched language-independent MathML formulae in 2020. Additionally, we describe our plans to improve the accessibility and discoverability of mathematical knowledge in Wikimedia projects further.},
  archivePrefix = {arXiv},
  eprint = {2003.09417},
  eprinttype = {arxiv},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\GG429ZQT\\SchubotzGMT20--tr--mathematical_formulae_in_wikimedia_projects_2020.pdf},
  keywords = {!ms_author,!nm_author,⚠️ Invalid DOI}
}

@inproceedings{SchubotzGSM18,
  ids = {Schubotz2018c,SchubotzGSM18a},
  title = {Improving the {{Representation}} and {{Conversion}} of {{Mathematical Formulae}} by {{Considering}} their {{Textual Context}}},
  booktitle = {Proceedings of the 18th {{ACM}}/{{IEEE}} on {{Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Schubotz, Moritz and {Greiner-Petter}, André and Scharpf, Philipp and Meuschke, Norman and Cohl, Howard S. and Gipp, Bela},
  year = {2018},
  month = jun,
  pages = {233--242},
  publisher = {{ACM}},
  address = {{Fort Worth, USA}},
  doi = {10.1145/3197026.3197058},
  url = {http://doi.acm.org/10.1145/3197026.3197058},
  abstract = {Mathematical formulae represent complex semantic information in a concise form. Especially in Science, Technology, Engineering, and Mathematics, mathematical formulae are crucial to communicate information, e.g., in scientific papers, and to perform computations using computer algebra systems. Enabling computers to access the information encoded in mathematical formulae requires machine-readable formats that can represent both the presentation and content, i.e., the semantics, of formulae. Exchanging such information between systems additionally requires conversion methods for mathematical representation formats. We analyze how the semantic enrichment of formulae improves the format conversion process and show that considering the textual context of formulae reduces the error rate of such conversions. Our main contributions are: (1) providing an openly available benchmark dataset for the mathematical format conversion task consisting of a newly created test collection, an extensive, manually curated gold standard and task-specific evaluation metrics; (2) performing a quantitative evaluation of state-of-the-art tools for mathematical format conversions; (3) presenting a new approach that considers the textual context of formulae to reduce the error rate for mathematical format conversions. Our benchmark dataset facilitates future research on mathematical format conversions as well as research on many problems in mathematical information retrieval. Because we annotated and linked all components of formulae, e.g., identifiers, operators and other entities, to Wikidata entries, the gold standard can, for instance, be used to train methods for formula concept discovery and recognition. Such methods can then be applied to improve mathematical information retrieval systems, e.g., for semantic formula search, recommendation of mathematical content, or detection of mathematical plagiarism.},
  biburl = {https://dblp.org/rec/bib/conf/jcdl/SchubotzGSMCG18},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  core = {0;Core Rank A*;http://portal.core.edu.au/conf-ranks/2085/},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\3GHGMSFF\\SchubotzGSM18--tr--improving_the_representation_and_conversion_of_mathematical_formulae_by_considering.pdf;D\:\\Zotero\\nmeuschke\\Data\\storage\\MA6ILN7H\\SchubotzGSM18--tr--improving_the_representation_and_conversion_of_mathematical_formulae_by_considering.pdf},
  isbn = {978-1-4503-5178-2},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,DFG1259-1,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {SchubotzGSMCG18},
  preprint = {https://arxiv.org/pdf/1804.04956.pdf},
  url_orig = {http://doi.acm.org/10.1145/3197026.3197058}
}

@incollection{SchubotzKMH17,
  ids = {SchubotzKMH17a},
  title = {Evaluating and {{Improving}} the {{Extraction}} of {{Mathematical Identifier Definitions}}},
  booktitle = {Experimental {{IR Meets Multilinguality}}, {{Multimodality}}, and {{Interaction}}},
  author = {Schubotz, Moritz and Krämer, Leonard and Meuschke, Norman and Hamborg, Felix and Gipp, Bela},
  editor = {Jones, Gareth J.F. and Lawless, Séamus and Gonzalo, Julio and Kelly, Liadh and Goeuriot, Lorraine and Mandl, Thomas and Cappellato, Linda and Ferro, Nicola},
  year = {2017},
  month = aug,
  volume = {10456 LNCS},
  pages = {82--94},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-65813-1_7},
  url = {http://link.springer.com/10.1007/978-3-319-65813-1_7},
  abstract = {Mathematical formulae in academic texts significantly contribute to the overall semantic content of such texts, especially in the fields of Science, Technology, Engineering and Mathematics. Knowing the definitions of the identifiers in mathematical formulae is essential to understand the semantics of the formulae. Similar to the sense-making process of human readers, mathematical information retrieval systems can analyze the text that surrounds formulae to extract the definitions of identifiers occurring in the formulae. Several approaches for extracting the definitions of mathematical identifiers from documents have been proposed in recent years. So far, these approaches have been evaluated using different collections and gold standard datasets, which prevented comparative performance assessments. To facilitate future research on the task of identifier definition extraction, we make three contributions. First, we provide an automated evaluation framework, which uses the dataset and gold standard of the NTCIR-11 Math Retrieval Wikipedia task. Second, we compare existing identifier extraction approaches using the developed evaluation framework. Third, we present a new identifier extraction approach that uses machine learning to combine the well-performing features of previous approaches. The new approach increases the precision of extracting identifier definitions from 17.85\% to 48.60\%, and increases the recall from 22.58\% to 28.06\%. The evaluation framework, the dataset and our source code are openly available at: https://ident.formulasearchengine.com.},
  biburl = {https://dblp.org/rec/bib/conf/clef/SchubotzKMHG17},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\68SQZMR5\\SchubotzKMH17--tr--evaluating_and_improving_the_extraction_of_mathematical_identifier_definitions.pdf;D\:\\Zotero\\nmeuschke\\Data\\storage\\GM4UYHGI\\SchubotzKMH17--tr--evaluating_and_improving_the_extraction_of_mathematical_identifier_definitions.pdf},
  isbn = {978-3-319-65812-4 978-3-319-65813-1},
  keywords = {!bg,!bg_author,!bg_preprint,!fh,!fh_author,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Schubotz2017},
  preprint = {https://ag-gipp.github.io/bib/preprints/schubotz2017.pdf},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  topic = {mathir}
}

@incollection{SchubotzMHC17,
  ids = {SchubotzMHC17a,SchubotzMHC17b},
  title = {{{VMEXT}}: {{A Visualization Tool}} for {{Mathematical Expression Trees}}},
  shorttitle = {{{VMEXT}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Schubotz, Moritz and Meuschke, Norman and Hepp, Thomas and Cohl, Howard S. and Gipp, Bela},
  editor = {Geuvers, Herman and England, Matthew and Hasan, Osman and Rabe, Florian and Teschke, Olaf},
  year = {2017},
  month = jul,
  volume = {10383 LNCS},
  pages = {340--355},
  publisher = {{Springer}},
  url = {https://doi.org/10.1007/978-3-319-62075-6_24},
  abstract = {Mathematical expressions can be represented as a tree consisting of terminal symbols, such as identifiers or numbers (leaf nodes), and functions or operators (non-leaf nodes). Expression trees are an important mechanism for storing and processing mathematical expressions as well as the most frequently used visualization of the structure of mathematical expressions. Typically, researchers and practitioners manually visualize expression trees using general-purpose tools. This approach is laborious, redundant, and error-prone. Manual visualizations represents a user’s notion of what the markup of an expression should be, but not necessarily what the actual markup is. This paper presents VMEXT – a free and open source tool to directly visualize expression trees from parallel  Open image in new window. VMEXT simultaneously visualizes the presentation elements and the semantic structure of mathematical expressions to enable users to quickly spot deficiencies in the Content  Open image in new window markup that does not affect the presentation of the expression. Identifying such discrepancies previously required reading the verbose and complex  Open image in new window markup. VMEXT also allows one to visualize similar and identical elements of two expressions. Visualizing expression similarity can support developers in designing retrieval approaches and enable improved interaction concepts for users of mathematical information retrieval systems. We demonstrate VMEXT’s visualizations in two web-based applications. The first application presents the visualizations alone. The second application shows a possible integration of the visualizations in systems for mathematical knowledge management and mathematical information retrieval. The application converts  Open image in new window input to parallel  Open image in new window, computes basic similarity measures for mathematical expressions, and visualizes the results using VMEXT.},
  biburl = {http://dblp.uni-trier.de/rec/bib/conf/mkm/SchubotzMHCG17},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\PXNHN3H9\\SchubotzMHC17--MS--vmext_a_visualization_tool_for_mathematical_expression_trees.pdf;D\:\\Zotero\\nmeuschke\\Data\\storage\\WXS2AL3U\\SchubotzMHC17--NM--vmext_a_visualization_tool_for_mathematical_expression_trees.pdf},
  isbn = {978-3-319-62074-9},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {vmext17},
  preprint = {https://arxiv.org/pdf/1707.03540.pdf},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  topic = {mathir}
}

@inproceedings{SchubotzMLG16,
  ids = {SchubotzMLG16a,SchubotzMLG16b},
  title = {Exploring the {{One}}-brain {{Barrier}}: a {{Manual Contribution}} to the {{NTCIR}}-12 {{Math Task}}},
  shorttitle = {Exploring the one-brain-barrier},
  booktitle = {Proceedings of the 12th {{NTCIR Conference}} on {{Evaluation}} of {{Information Access Technologies}}},
  author = {Schubotz, Moritz and Meuschke, Norman and Leich, Marcus and Gipp, Bela},
  year = {2016},
  month = jun,
  doi = {10.5281/zenodo.3547436},
  url = {http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings12/pdf/ntcir/MathIR/02-NTCIR12-MathIR-SchubotzM.pdf},
  abstract = {This paper compares the search capabilities of a single human brain supported by the text search built into Wikipedia with state-of-the-art math search systems. To achieve this, we compare results of manual Wikipedia searches with the aggregated and assessed results of all systems participating in the NTCIR-12 MathIR Wikipedia Task. For 26 of the 30 topics, the average relevance score of our manually retrieved results exceeded the average relevance score of other participants by more than one standard deviation. However, math search engines at large achieved better recall and retrieved highly relevant results that our ‘single-brain system’ missed for 12 topics. By categorizing the topics of NTCIR-12 into six types of queries, we observe a particular strength of math search engines to answer queries of the types ‘definition lookup’ and ‘application look-up’. However, we see the low precision of current math search engines as the main challenge that prevents their wide-spread adoption in STEM research. By combining our results with highly relevant results of all other participants, we compile a new gold standard dataset and a dataset of duplicate content items. We discuss how the two datasets can be used to improve the query formulation and content augmentation capabilities of match search engines in the future},
  biburl = {https://dblp.org/rec/bib/conf/ntcir/SchubotzMLG16},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\DIZGR8DJ\\SchubotzMLG16--tr--exploring_the_one-brain_barrier_a_manual_contribution_to_the_ntcir-12_math_task.pdf;D\:\\Zotero\\nmeuschke\\Data\\storage\\PR7Z9Y3N\\SchubotzMLG16--tr--exploring_the_one-brain_barrier_a_manual_contribution_to_the_ntcir-12_math_task.pdf},
  jabref-groups = {phd-m},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  maintitle = {Exploring the one-brain-barrier},
  oldkey = {Schubotz2016b},
  owner = {Moritz},
  preprint = {https://ag-gipp.github.io/bib/preprints/schubotz2016b.pdf},
  topic = {mathir}
}

@inproceedings{SchubotzTSM19,
  ids = {SchubotzTSM19a},
  title = {Forms of {{Plagiarism}} in {{Digital Mathematical Libraries}}},
  booktitle = {Proceedings {{International Conference}} on {{Intelligent Computer Mathematics}}},
  author = {Schubotz, Moritz and Teschke, Olaf and Stange, Vincent and Meuschke, Norman and Gipp, Bela},
  year = {2019},
  month = jul,
  volume = {11617 LNCS},
  pages = {258--274},
  address = {{Czech Republic}},
  doi = {10.1007/978-3-030-23250-4_18},
  abstract = {We report on an exploratory analysis of the forms of plagiarism observable in mathematical publications, which we identified by investigating editorial notes from zbMATH. While most cases we encountered were simple copies of earlier work, we also identified several forms of disguised plagiarism. We investigated 11 cases in detail and evaluate how current plagiarism detection systems perform in identifying these cases. Moreover, we describe the steps required to discover these and potentially undiscovered cases in the future.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\RWGBG93T\\SchubotzTSM19--MS--forms_of_plagiarism_in_digital_mathematical_libraries.pdf},
  keywords = {!bg,!bg_author,!bg_preprint,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,DFG1259-1,jabref_imp1_clean,nm_diss_litrev,old_tex_field_preprint},
  oldkey = {Schubotz2019},
  preprint = {https://ag-gipp.github.io/bib/preprints/schubotz2019.pdf},
  topic = {pd}
}

@inproceedings{SchwarzerBSM17,
  ids = {SchwarzerBSM17a},
  title = {Citolytics: {{A Link}}-based {{Recommender System}} for {{Wikipedia}}},
  shorttitle = {Citolytics},
  booktitle = {Proceedings of the 11th {{ACM Conference}} on {{Recommender Systems}} ({{RecSys}})},
  author = {Schwarzer, Malte and Breitinger, Corinna and Schubotz, Moritz and Meuschke, Norman and Gipp, Bela},
  year = {2017},
  month = aug,
  pages = {360--361},
  publisher = {{ACM}},
  doi = {10.1145/3109859.3109981},
  abstract = {We present Citolytics - a novel link-based recommendation system for Wikipedia articles. In a preliminary study, Citolytics achieved promising results compared to the widely used text-based approach of Apache Lucene's MoreLikeThis (MLT). In this demo paper, we describe how we plan to integrate Citolytics into the Wikipedia infrastructure by using Elasticsearch and Apache Flink to serve recommendations for Wikipedia articles. Additionally, we propose a large-scale online evaluation design using the Wikipedia Android app. Working with Wikipedia data has several unique advantages. First, the availability of a very large user sample contributes to statistically significant results. Second, the openness of Wikipedia's architecture allows making our source code and evaluation data public, thus benefiting other researchers. If link-based recommendations show promise in our online evaluation, a deployment of the presented system within Wikipedia would have a far-reaching impact on Wikipedia's more than 30 million users.},
  biburl = {https://dblp.org/rec/bib/conf/recsys/SchwarzerBSMG17},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\4N59G9PN\\SchwarzerBSM17--tr--citolytics_a_link-based_recommender_system_for_wikipedia.pdf;D\:\\Zotero\\nmeuschke\\Data\\storage\\BC2WXNB9\\SchwarzerBSM17--MS--citolytics_a_link-based_recommender_system_for_wikipedia.pdf},
  isbn = {978-1-4503-4652-8},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Schwarzer2017},
  preprint = {https://ag-gipp.github.io/bib/preprints/schwarzer2017.pdf},
  topic = {rec}
}

@inproceedings{SchwarzerSMB16,
  ids = {SchwarzerSMB16a},
  title = {Evaluating {{Link}}-based {{Recommendations}} for {{Wikipedia}}},
  booktitle = {Proceedings of the 16th {{Annual International ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Schwarzer, Malte and Schubotz, Moritz and Meuschke, Norman and Breitinger, Corinna and Markl, Volker and Gipp, Bela},
  year = {2016},
  month = jun,
  pages = {191--200},
  publisher = {{ACM}},
  address = {{Newark, New Jersey, USA}},
  doi = {10.1145/2910896.2910908},
  abstract = {Literature recommender systems support users in filtering the vast and increasing number of documents in digital libraries and on the Web. For academic literature, research has proven the ability of citation-based document similarity measures, such as Co-Citation (CoCit), or Co-Citation Proximity Analysis (CPA) to improve recommendation quality. In this paper, we report on the first large-scale investigation of the performance of the CPA approach in generating literature recommendations for Wikipedia, which is fundamentally different from the academic literature domain. We analyze links instead of citations to generate article recommendations. We evaluate CPA, CoCit, and the Apache Lucene MoreLikeThis (MLT) function, which represents a traditional text-based similarity measure. We use two datasets of 779,716 and 2.57 million Wikipedia articles, the Big Data processing framework Apache Flink, and a ten-node computing cluster. To enable our large-scale evaluation, we derive two quasi-gold standards from the links in Wikipedia's "See also" sections and a comprehensive Wikipedia clickstream dataset.

Our results show that the citation-based measures CPA and CoCit have complementary strengths compared to the text-based MLT measure. While MLT performs well in identifying narrowly similar articles that share similar words and structure, the citation- based measures are better able to identify topically related information, such as information on the city of a certain university or other technical universities in the region. The CPA approach, which consistently outperformed CoCit, is better suited for identifying a broader spectrum of related articles, as well as popular articles that typically exhibit a higher quality. Additional benefits of the CPA approach are its lower runtime requirements and its language-independence that allows for a cross-language retrieval of articles. We present a manual analysis of exemplary articles to demonstrate and discuss our findings. The raw data and source code of our study, together with a manual on how to use them, are openly available at: https://github.com/wikimedia/citolytics},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  core = {A*},
  file = {D\:\\Zotero\\nmeuschke\\Data\\storage\\J2FJIFUQ\\SchwarzerSMB16--tr--evaluating_link-based_recommendations_for_wikipedia.pdf},
  isbn = {978-1-4503-4229-2},
  keywords = {!bg,!bg_author,!bg_preprint,!cb,!cb_author,!ms,!ms_author,!ms_cv,!ms_preprint,!nm,!nm_author,!nm_preprint,jabref_imp1_clean,old_tex_field_preprint},
  oldkey = {Schwarzer2016},
  preprint = {https://ag-gipp.github.io/bib/preprints/schwarzer2016.pdf},
  topic = {rec}
}


